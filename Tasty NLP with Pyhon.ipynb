{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\schapira.d\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from random import sample\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "# nltk models\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#spaCy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# gensim models\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# Visualize topics\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meet spaCy - \"It's minimal and opinionated\"\n",
    "\n",
    "spaCy is a free, **open-source** library for advanced **Natural Language Processing (NLP)** in Python.\n",
    "\n",
    "Features:\n",
    "\n",
    "-  Tokenization\n",
    "-  POS Tagging\n",
    "-  Dependency Parsing\n",
    "-  Lemmatization\n",
    "-  Sentence Detection\n",
    "-  Entity Recognition\n",
    "\n",
    "And more...\n",
    "\n",
    "https://spacy.io/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Best restaurant in Newcastle. Delicious cocktails and it has a really friendly atmosphere."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Best restaurant in Newcastle. Delicious cocktails and it has a really friendly atmosphere.'\n",
    "\n",
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_pos</th>\n",
       "      <th>token_entity</th>\n",
       "      <th>token_is_stop</th>\n",
       "      <th>token_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best</td>\n",
       "      <td>good</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[-1.6054, 4.53702, -0.672109, -2.05529, -1.600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>restaurant</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[1.33561, 0.368323, 2.64288, 0.373385, -2.4033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>[1.44569, 1.10417, -0.406471, 1.28426, 1.65253...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Newcastle</td>\n",
       "      <td>newcastle</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>GPE</td>\n",
       "      <td>False</td>\n",
       "      <td>[-2.38429, 0.409086, 3.07898, 0.594176, -0.754...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[0.474397, 1.99392, 2.95767, -0.329908, 1.3399...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Delicious</td>\n",
       "      <td>delicious</td>\n",
       "      <td>PROPN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[-3.31449, 3.54701, -0.886751, 2.62596, -3.169...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cocktails</td>\n",
       "      <td>cocktail</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[0.890365, 3.60857, -1.63054, -2.22932, 1.359,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>[0.74245, -1.03995, -0.239206, -1.88797, 2.333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>it</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>[-1.6597, 0.930871, 3.74128, 2.16395, -2.18548...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>has</td>\n",
       "      <td>have</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>[-3.41011, 2.17194, -1.65569, -1.99658, 3.2991...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>[2.64929, -2.16059, 1.57773, 7.26103, 6.65484,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>really</td>\n",
       "      <td>really</td>\n",
       "      <td>ADV</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>[0.0771762, -1.56962, -2.69741, 4.12226, 2.141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>friendly</td>\n",
       "      <td>friendly</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[-1.62595, -1.60285, -1.74331, 0.34037, -0.901...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>atmosphere</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[0.667868, 0.940877, 1.00614, 1.26148, -1.2045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[3.50395, -0.748438, 1.81277, -1.85407, 1.1134...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text token_lemma token_pos token_entity  token_is_stop  \\\n",
       "0         Best        good       ADJ                       False   \n",
       "1   restaurant  restaurant      NOUN                       False   \n",
       "2           in          in       ADP                        True   \n",
       "3    Newcastle   newcastle     PROPN          GPE          False   \n",
       "4            .           .     PUNCT                       False   \n",
       "5    Delicious   delicious     PROPN                       False   \n",
       "6    cocktails    cocktail      NOUN                       False   \n",
       "7          and         and     CCONJ                        True   \n",
       "8           it      -PRON-      PRON                        True   \n",
       "9          has        have      VERB                        True   \n",
       "10           a           a       DET                        True   \n",
       "11      really      really       ADV                        True   \n",
       "12    friendly    friendly       ADJ                       False   \n",
       "13  atmosphere  atmosphere      NOUN                       False   \n",
       "14           .           .     PUNCT                       False   \n",
       "\n",
       "                                            token_vec  \n",
       "0   [-1.6054, 4.53702, -0.672109, -2.05529, -1.600...  \n",
       "1   [1.33561, 0.368323, 2.64288, 0.373385, -2.4033...  \n",
       "2   [1.44569, 1.10417, -0.406471, 1.28426, 1.65253...  \n",
       "3   [-2.38429, 0.409086, 3.07898, 0.594176, -0.754...  \n",
       "4   [0.474397, 1.99392, 2.95767, -0.329908, 1.3399...  \n",
       "5   [-3.31449, 3.54701, -0.886751, 2.62596, -3.169...  \n",
       "6   [0.890365, 3.60857, -1.63054, -2.22932, 1.359,...  \n",
       "7   [0.74245, -1.03995, -0.239206, -1.88797, 2.333...  \n",
       "8   [-1.6597, 0.930871, 3.74128, 2.16395, -2.18548...  \n",
       "9   [-3.41011, 2.17194, -1.65569, -1.99658, 3.2991...  \n",
       "10  [2.64929, -2.16059, 1.57773, 7.26103, 6.65484,...  \n",
       "11  [0.0771762, -1.56962, -2.69741, 4.12226, 2.141...  \n",
       "12  [-1.62595, -1.60285, -1.74331, 0.34037, -0.901...  \n",
       "13  [0.667868, 0.940877, 1.00614, 1.26148, -1.2045...  \n",
       "14  [3.50395, -0.748438, 1.81277, -1.85407, 1.1134...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.text for token in doc]\n",
    "token_pos = [token.pos_ for token in doc]\n",
    "token_lemma = [token.lemma_ for token in doc]\n",
    "token_entity = [token.ent_type_ for token in doc]\n",
    "token_stop = [token.is_stop for token in doc]\n",
    "token_vec = [token.vector for token in doc]\n",
    "\n",
    "headers = ['token_text','token_lemma','token_pos','token_entity','token_is_stop','token_vec']\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_lemma, token_pos, token_entity,token_stop,token_vec)),columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_head</th>\n",
       "      <th>token_children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>restaurant</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>[Best, in, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>[Newcastle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Newcastle</td>\n",
       "      <td>in</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Delicious</td>\n",
       "      <td>cocktails</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cocktails</td>\n",
       "      <td>has</td>\n",
       "      <td>[Delicious, and, it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and</td>\n",
       "      <td>cocktails</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>it</td>\n",
       "      <td>cocktails</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>has</td>\n",
       "      <td>has</td>\n",
       "      <td>[cocktails, atmosphere, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a</td>\n",
       "      <td>atmosphere</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>really</td>\n",
       "      <td>friendly</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>friendly</td>\n",
       "      <td>atmosphere</td>\n",
       "      <td>[really]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>has</td>\n",
       "      <td>[a, friendly]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.</td>\n",
       "      <td>has</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text  token_head              token_children\n",
       "0         Best  restaurant                          []\n",
       "1   restaurant  restaurant               [Best, in, .]\n",
       "2           in  restaurant                 [Newcastle]\n",
       "3    Newcastle          in                          []\n",
       "4            .  restaurant                          []\n",
       "5    Delicious   cocktails                          []\n",
       "6    cocktails         has        [Delicious, and, it]\n",
       "7          and   cocktails                          []\n",
       "8           it   cocktails                          []\n",
       "9          has         has  [cocktails, atmosphere, .]\n",
       "10           a  atmosphere                          []\n",
       "11      really    friendly                          []\n",
       "12    friendly  atmosphere                    [really]\n",
       "13  atmosphere         has               [a, friendly]\n",
       "14           .         has                          []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_head = [token.head for token in doc]\n",
    "token_children = [list(token.children) for token in doc]\n",
    "\n",
    "headers_ = ['token_text','token_head','token_children']\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_head, token_children)),columns=headers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\schapira.d\\AppData\\Local\\Continuum\\Anaconda3\\lib\\runpy.py:193: DeprecationWarning: Positional arguments to Doc.merge are deprecated. Instead, use the keyword arguments, for example tag=, lemma= or ent_type=.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\schapira.d\\AppData\\Local\\Continuum\\Anaconda3\\lib\\runpy.py:193: DeprecationWarning: Positional arguments to Doc.merge are deprecated. Instead, use the keyword arguments, for example tag=, lemma= or ent_type=.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1220\" height=\"317.0\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Best</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">restaurant</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">Newcastle.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">Delicious</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">cocktails</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">it</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">has</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">really</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">friendly</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">atmosphere.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,182.0 C70,137.0 125.0,137.0 125.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,184.0 L62,172.0 78,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M160,182.0 C160,137.0 215.0,137.0 215.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M215.0,184.0 L223.0,172.0 207.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M250,182.0 C250,137.0 305.0,137.0 305.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M305.0,184.0 L313.0,172.0 297.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M430,182.0 C430,137.0 485.0,137.0 485.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M430,184.0 L422,172.0 438,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M520,182.0 C520,47.0 765.0,47.0 765.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M520,184.0 L512,172.0 528,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M520,182.0 C520,137.0 575.0,137.0 575.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,184.0 L583.0,172.0 567.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M520,182.0 C520,92.0 670.0,92.0 670.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670.0,184.0 L678.0,172.0 662.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M880,182.0 C880,47.0 1125.0,47.0 1125.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M880,184.0 L872,172.0 888,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M970,182.0 C970,137.0 1025.0,137.0 1025.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M970,184.0 L962,172.0 978,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M1060,182.0 C1060,137.0 1115.0,137.0 1115.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1060,184.0 L1052,172.0 1068,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-10\" stroke-width=\"2px\" d=\"M790,182.0 C790,2.0 1130.0,2.0 1130.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-10\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1130.0,184.0 L1138.0,172.0 1122.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Best restaurant,\n",
       " Newcastle,\n",
       " Delicious cocktails,\n",
       " it,\n",
       " a really friendly atmosphere]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Best restaurant in Newcastle.,\n",
       " Delicious cocktails and it has a really friendly atmosphere.]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec\n",
    "\n",
    "300 dimensions doc2vec as average of token vectors trained using GloVe on Common Crawl dataset\n",
    "\n",
    "https://en.wikipedia.org/wiki/Common_Crawl\n",
    "\n",
    "https://spacy.io/models/en#section-en_vectors_web_lg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = ['Pork is amazing','Sausage was great','Data Science made simple','Physics studies laws of the universe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_text</th>\n",
       "      <th>doc_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pork is amazing</td>\n",
       "      <td>[-0.332147, 0.185507, 0.2583, 0.130159, 0.1209...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sausage was great</td>\n",
       "      <td>[-0.206415, 0.324179, 0.18584, 0.0150927, -7.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science made simple</td>\n",
       "      <td>[-0.26445, 0.0599757, -0.181192, 0.0580605, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Physics studies laws of the universe</td>\n",
       "      <td>[0.0194767, 0.0151591, -0.171293, -0.183741, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               doc_text  \\\n",
       "0                       Pork is amazing   \n",
       "1                     Sausage was great   \n",
       "2              Data Science made simple   \n",
       "3  Physics studies laws of the universe   \n",
       "\n",
       "                                             doc_vec  \n",
       "0  [-0.332147, 0.185507, 0.2583, 0.130159, 0.1209...  \n",
       "1  [-0.206415, 0.324179, 0.18584, 0.0150927, -7.1...  \n",
       "2  [-0.26445, 0.0599757, -0.181192, 0.0580605, -0...  \n",
       "3  [0.0194767, 0.0151591, -0.171293, -0.183741, -...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def doc2vec(docs):\n",
    "\n",
    "    \"\"\"\n",
    "    Get doc2vec representations of docs using spaCy pre-trained word vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_text = []\n",
    "    doc_vec = []\n",
    "\n",
    "    for doc in nlp.pipe(docs):\n",
    "        doc_text.append(doc.text)\n",
    "        doc_vec.append(doc.vector)\n",
    "    \n",
    "    headers = ['doc_text','doc_vec']\n",
    "\n",
    "    return pd.DataFrame(list(zip(doc_text, doc_vec)),columns=headers)\n",
    "\n",
    "df = doc2vec(docs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pork is amazing</th>\n",
       "      <th>Sausage was great</th>\n",
       "      <th>Data Science made simple</th>\n",
       "      <th>Physics studies laws of the universe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pork is amazing</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.816263</td>\n",
       "      <td>0.543974</td>\n",
       "      <td>0.441451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sausage was great</th>\n",
       "      <td>0.816263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.508645</td>\n",
       "      <td>0.422934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data Science made simple</th>\n",
       "      <td>0.543974</td>\n",
       "      <td>0.508645</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.744686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Physics studies laws of the universe</th>\n",
       "      <td>0.441451</td>\n",
       "      <td>0.422934</td>\n",
       "      <td>0.744686</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Pork is amazing  Sausage was great  \\\n",
       "Pork is amazing                              1.000000           0.816263   \n",
       "Sausage was great                            0.816263           1.000000   \n",
       "Data Science made simple                     0.543974           0.508645   \n",
       "Physics studies laws of the universe         0.441451           0.422934   \n",
       "\n",
       "                                      Data Science made simple  \\\n",
       "Pork is amazing                                       0.543974   \n",
       "Sausage was great                                     0.508645   \n",
       "Data Science made simple                              1.000000   \n",
       "Physics studies laws of the universe                  0.744686   \n",
       "\n",
       "                                      Physics studies laws of the universe  \n",
       "Pork is amazing                                                   0.441451  \n",
       "Sausage was great                                                 0.422934  \n",
       "Data Science made simple                                          0.744686  \n",
       "Physics studies laws of the universe                              1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "docvecs = df['doc_vec'].tolist()\n",
    "cos_sim = cosine_similarity(docvecs)\n",
    "df_sim = pd.DataFrame(cos_sim,columns=df['doc_text'].tolist(),index=df['doc_text'].tolist())\n",
    "df_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build an NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>categories</th>\n",
       "      <th>text</th>\n",
       "      <th>stars_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vXEZ-r6fah-5Fjt3a6c-Gw</td>\n",
       "      <td>\"The Cheesecake Factory\"</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>American (Traditional);Desserts;Food;American ...</td>\n",
       "      <td>One of my favorite places too take the kids wh...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                      name        city  \\\n",
       "0  vXEZ-r6fah-5Fjt3a6c-Gw  \"The Cheesecake Factory\"  Pittsburgh   \n",
       "\n",
       "                                          categories  \\\n",
       "0  American (Traditional);Desserts;Food;American ...   \n",
       "\n",
       "                                                text  stars_x  \n",
       "0  One of my favorite places too take the kids wh...        4  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "path = 'C:\\\\Users\\\\schapira.d\\\\Desktop\\\\Data Science Meetup\\\\yelp_reviews_1M.csv'\n",
    "reviews_df = pd.read_csv(path,encoding='utf-8')\n",
    "reviews = reviews_df['text'].fillna('').tolist()\n",
    "ratings = reviews_df['stars_x'].tolist()\n",
    "reviews_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm',disable=['parser','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def TextPreprocessSpaCy(docs):\n",
    "    text = []\n",
    "    pos = ['ADJ','NOUN']\n",
    "    stop = ['-pron-']\n",
    "    for doc in nlp.pipe(docs):\n",
    "        tokens = [token.lemma_.lower() for token in doc if token.pos_ in pos and token.is_stop == False] \n",
    "        tokens = [i for i in tokens if i not in stop]\n",
    "        text.append(tokens)\n",
    "                \n",
    "    return text\n",
    "\n",
    "def TextPreprocessNLTK(docs):\n",
    "    text = []\n",
    "    pos = ['JJ','JJR','JJS','NN','NNS']\n",
    "    stop = stopwords.words('english')\n",
    "    lemma = WordNetLemmatizer()\n",
    "        \n",
    "    for i in docs:\n",
    "        tokens = word_tokenize(i.lower()) # tokenize\n",
    "        tokens = pos_tag(tokens) # POS tagger\n",
    "        tokens = [i[0] for i in tokens if i[1] in pos] # POS filter\n",
    "        tokens = [i for i in tokens if i not in string.punctuation] # remove punctuation\n",
    "        tokens = [i for i in tokens if i not in stop] # remove stopwords\n",
    "        tokens = [lemma.lemmatize(i) for i in tokens] # lemmatize\n",
    "        text.append(tokens)\n",
    "        \n",
    "    return text\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'place', 'sushi', 'area', 'everything', 'fresh', 'chef', 'pride', 'piece', 'sushi'] \n",
      " --SpaCy Run time: 7.722013473510742s\n",
      "\n",
      "['best', 'place', 'sushi', 'area', 'everything', 'fresh', 'chef', 'piece', 'sushi'] \n",
      " --NLTK Run time: 11.044819355010986s\n",
      "\n",
      "By far the best place to get sushi in the area.  Everything is fresh and you can tell that the chef takes pride in each piece of sushi that he creates.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "SpaCy = TextPreprocessSpaCy(reviews[0:1000])\n",
    "end = time.time()\n",
    "print(\"{} \\n --SpaCy Run time: {}s\".format(SpaCy[40],(end-start)))\n",
    "\n",
    "start = time.time()\n",
    "NLTK = TextPreprocessNLTK(reviews[0:1000])\n",
    "end = time.time()\n",
    "print(\"\\n{} \\n --NLTK Run time: {}s\".format(NLTK[40],(end-start)))\n",
    "\n",
    "print(\"\\n{}\".format(reviews[40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 8min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reviews_spacy = TextPreprocessSpaCy(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path ='C:\\\\Users\\\\schapira.d\\\\Desktop\\\\Data Science Meetup\\\\tokens.pkl' \n",
    "with open(path,'wb') as f:\n",
    "    pickle.dump(reviews_spacy, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#load preprocessed dataset:\n",
    "import pickle\n",
    "path_tokens ='C:\\\\Users\\\\schapira.d\\\\Desktop\\\\Data Science Meetup\\\\tokens_spacy.pkl' \n",
    "with open(path_tokens,'rb') as f:\n",
    "    reviews_spacy = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrases model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Phrases Modelling\n",
    "bigram_model = Phrases(reviews_spacy,min_count=25)\n",
    "bigram_phraser = Phraser(bigram_model)\n",
    "trigram_model = Phrases(bigram_phraser[reviews_spacy],min_count=25)\n",
    "trigram_phraser = Phraser(trigram_model)\n",
    "\n",
    "reviews_trigram = list(trigram_phraser[bigram_phraser[reviews_spacy]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path ='C:\\\\Users\\\\schapira.d\\\\Desktop\\\\Data Science Meetup\\\\phrases.pkl' \n",
    "with open(path,'wb') as f:\n",
    "    pickle.dump(reviews_trigram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing', 'lunch', 'spot', 'endless', 'option', 'daily_special', 'new', 'delicious', 'option', 'salad', 'lover', 'burger', 'sandwich', 'wrap', 'pasta', 'breakfast', 'variety', 'fruit', 'snack', 'shake', 'anyone', 'quaint', 'spot', 'indoor', 'guy', 'great', 'taste']\n"
     ]
    }
   ],
   "source": [
    "print(reviews_trigram[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrases</th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>categories</th>\n",
       "      <th>text</th>\n",
       "      <th>stars_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>favorite</td>\n",
       "      <td>vXEZ-r6fah-5Fjt3a6c-Gw</td>\n",
       "      <td>\"The Cheesecake Factory\"</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>American (Traditional);Desserts;Food;American (New);Restaurants</td>\n",
       "      <td>One of my favorite places too take the kids when there home from college or during the holidays ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>place</td>\n",
       "      <td>vXEZ-r6fah-5Fjt3a6c-Gw</td>\n",
       "      <td>\"The Cheesecake Factory\"</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>American (Traditional);Desserts;Food;American (New);Restaurants</td>\n",
       "      <td>One of my favorite places too take the kids when there home from college or during the holidays ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kid</td>\n",
       "      <td>vXEZ-r6fah-5Fjt3a6c-Gw</td>\n",
       "      <td>\"The Cheesecake Factory\"</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>American (Traditional);Desserts;Food;American (New);Restaurants</td>\n",
       "      <td>One of my favorite places too take the kids when there home from college or during the holidays ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>college</td>\n",
       "      <td>vXEZ-r6fah-5Fjt3a6c-Gw</td>\n",
       "      <td>\"The Cheesecake Factory\"</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>American (Traditional);Desserts;Food;American (New);Restaurants</td>\n",
       "      <td>One of my favorite places too take the kids when there home from college or during the holidays ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>holiday</td>\n",
       "      <td>vXEZ-r6fah-5Fjt3a6c-Gw</td>\n",
       "      <td>\"The Cheesecake Factory\"</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>American (Traditional);Desserts;Food;American (New);Restaurants</td>\n",
       "      <td>One of my favorite places too take the kids when there home from college or during the holidays ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Phrases             business_id                      name        city  \\\n",
       "0  favorite  vXEZ-r6fah-5Fjt3a6c-Gw  \"The Cheesecake Factory\"  Pittsburgh   \n",
       "1     place  vXEZ-r6fah-5Fjt3a6c-Gw  \"The Cheesecake Factory\"  Pittsburgh   \n",
       "2       kid  vXEZ-r6fah-5Fjt3a6c-Gw  \"The Cheesecake Factory\"  Pittsburgh   \n",
       "3   college  vXEZ-r6fah-5Fjt3a6c-Gw  \"The Cheesecake Factory\"  Pittsburgh   \n",
       "4   holiday  vXEZ-r6fah-5Fjt3a6c-Gw  \"The Cheesecake Factory\"  Pittsburgh   \n",
       "\n",
       "                                                        categories  \\\n",
       "0  American (Traditional);Desserts;Food;American (New);Restaurants   \n",
       "1  American (Traditional);Desserts;Food;American (New);Restaurants   \n",
       "2  American (Traditional);Desserts;Food;American (New);Restaurants   \n",
       "3  American (Traditional);Desserts;Food;American (New);Restaurants   \n",
       "4  American (Traditional);Desserts;Food;American (New);Restaurants   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  One of my favorite places too take the kids when there home from college or during the holidays ...   \n",
       "1  One of my favorite places too take the kids when there home from college or during the holidays ...   \n",
       "2  One of my favorite places too take the kids when there home from college or during the holidays ...   \n",
       "3  One of my favorite places too take the kids when there home from college or during the holidays ...   \n",
       "4  One of my favorite places too take the kids when there home from college or during the holidays ...   \n",
       "\n",
       "   stars_x  \n",
       "0        4  \n",
       "1        4  \n",
       "2        4  \n",
       "3        4  \n",
       "4        4  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transforming to df for unstacking and join\n",
    "df_phrases = pd.DataFrame({\"Phrases\" : reviews_trigram}).head(100000)\n",
    "\n",
    "#Unstacking...\n",
    "df = pd.DataFrame({'Index':np.repeat(df_phrases.index.values, df_phrases.Phrases.str.len()),\n",
    "              'Phrases':np.concatenate(df_phrases.Phrases.values)})\n",
    "df.set_index('Index', inplace = True)\n",
    "\n",
    "#Joining with full data\n",
    "reviews_phrases = pd.merge(df,reviews_df.head(100000),left_index=True,right_index=True).reset_index(drop=True)\n",
    "pd.to_numeric(reviews_phrases.stars_x)\n",
    "\n",
    "reviews_phrases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pivot phrases by avg. rating\n",
    "phrases = pd.pivot_table(reviews_phrases, index='Phrases',aggfunc={'stars_x':[np.mean,len]})\n",
    "phrases.columns = phrases.columns.to_series().str.join('_')\n",
    "df = phrases.sort_values('stars_x_mean',ascending=False)\n",
    "df.columns = ['term_frequency','avg_rating']\n",
    "df = df[df.term_frequency > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_frequency</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phrases</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recommend</th>\n",
       "      <td>376</td>\n",
       "      <td>4.643617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love_love</th>\n",
       "      <td>133</td>\n",
       "      <td>4.639098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hidden_gem</th>\n",
       "      <td>423</td>\n",
       "      <td>4.626478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personal_favorite</th>\n",
       "      <td>152</td>\n",
       "      <td>4.611842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gem</th>\n",
       "      <td>857</td>\n",
       "      <td>4.588098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>453</td>\n",
       "      <td>4.576159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth_penny</th>\n",
       "      <td>165</td>\n",
       "      <td>4.575758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impeccable</th>\n",
       "      <td>352</td>\n",
       "      <td>4.571023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incredible</th>\n",
       "      <td>1432</td>\n",
       "      <td>4.567039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phenomenal</th>\n",
       "      <td>724</td>\n",
       "      <td>4.563536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hooked</th>\n",
       "      <td>142</td>\n",
       "      <td>4.549296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfection</th>\n",
       "      <td>1246</td>\n",
       "      <td>4.534510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolute_favorite</th>\n",
       "      <td>126</td>\n",
       "      <td>4.523810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>divine</th>\n",
       "      <td>302</td>\n",
       "      <td>4.513245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scrumptious</th>\n",
       "      <td>197</td>\n",
       "      <td>4.507614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heavenly</th>\n",
       "      <td>163</td>\n",
       "      <td>4.484663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>13197</td>\n",
       "      <td>4.480564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>find</th>\n",
       "      <td>319</td>\n",
       "      <td>4.476489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exquisite</th>\n",
       "      <td>129</td>\n",
       "      <td>4.465116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>superb</th>\n",
       "      <td>598</td>\n",
       "      <td>4.459866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   term_frequency  avg_rating\n",
       "Phrases                                      \n",
       "recommend                     376    4.643617\n",
       "love_love                     133    4.639098\n",
       "hidden_gem                    423    4.626478\n",
       "personal_favorite             152    4.611842\n",
       "gem                           857    4.588098\n",
       "best                          453    4.576159\n",
       "worth_penny                   165    4.575758\n",
       "impeccable                    352    4.571023\n",
       "incredible                   1432    4.567039\n",
       "phenomenal                    724    4.563536\n",
       "hooked                        142    4.549296\n",
       "perfection                   1246    4.534510\n",
       "absolute_favorite             126    4.523810\n",
       "divine                        302    4.513245\n",
       "scrumptious                   197    4.507614\n",
       "heavenly                      163    4.484663\n",
       "amazing                     13197    4.480564\n",
       "find                          319    4.476489\n",
       "exquisite                     129    4.465116\n",
       "superb                        598    4.459866"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top phrases with highest avg. rating\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_frequency</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phrases</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shitty</th>\n",
       "      <td>114</td>\n",
       "      <td>1.956140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lousy</th>\n",
       "      <td>106</td>\n",
       "      <td>1.952830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>response</th>\n",
       "      <td>384</td>\n",
       "      <td>1.950521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horrendous</th>\n",
       "      <td>105</td>\n",
       "      <td>1.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trash</th>\n",
       "      <td>231</td>\n",
       "      <td>1.922078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nasty</th>\n",
       "      <td>420</td>\n",
       "      <td>1.890476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tasteless</th>\n",
       "      <td>543</td>\n",
       "      <td>1.858195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inedible</th>\n",
       "      <td>323</td>\n",
       "      <td>1.820433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terrible</th>\n",
       "      <td>2225</td>\n",
       "      <td>1.788315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pathetic</th>\n",
       "      <td>118</td>\n",
       "      <td>1.779661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rude</th>\n",
       "      <td>1911</td>\n",
       "      <td>1.776033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awful</th>\n",
       "      <td>1128</td>\n",
       "      <td>1.763298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apology</th>\n",
       "      <td>355</td>\n",
       "      <td>1.760563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horrible</th>\n",
       "      <td>2039</td>\n",
       "      <td>1.680235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unacceptable</th>\n",
       "      <td>164</td>\n",
       "      <td>1.621951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgusting</th>\n",
       "      <td>666</td>\n",
       "      <td>1.575075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filthy</th>\n",
       "      <td>152</td>\n",
       "      <td>1.572368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unprofessional</th>\n",
       "      <td>206</td>\n",
       "      <td>1.543689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poisoning</th>\n",
       "      <td>215</td>\n",
       "      <td>1.446512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refund</th>\n",
       "      <td>206</td>\n",
       "      <td>1.310680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                term_frequency  avg_rating\n",
       "Phrases                                   \n",
       "shitty                     114    1.956140\n",
       "lousy                      106    1.952830\n",
       "response                   384    1.950521\n",
       "horrendous                 105    1.942857\n",
       "trash                      231    1.922078\n",
       "nasty                      420    1.890476\n",
       "tasteless                  543    1.858195\n",
       "inedible                   323    1.820433\n",
       "terrible                  2225    1.788315\n",
       "pathetic                   118    1.779661\n",
       "rude                      1911    1.776033\n",
       "awful                     1128    1.763298\n",
       "apology                    355    1.760563\n",
       "horrible                  2039    1.680235\n",
       "unacceptable               164    1.621951\n",
       "disgusting                 666    1.575075\n",
       "filthy                     152    1.572368\n",
       "unprofessional             206    1.543689\n",
       "poisoning                  215    1.446512\n",
       "refund                     206    1.310680"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top phrases with lowest avg. rating\n",
    "df.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "Using gensim implementation of Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Link to original paper by David Blei, Andrew Ng, Michael Jordan (2003) Journal of Machine Learning Research\n",
    "\n",
    "http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# turn tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(reviews_trigram)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "dictionary.compactify()\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(i) for i in reviews_trigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4h 23min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#where the magic happens\n",
    "lda_model = gensim.models.ldamulticore.LdaMulticore(corpus,\n",
    "                                                    num_topics=50, \n",
    "                                                    id2word=dictionary, \n",
    "                                                    workers=3, passes=10)\n",
    "\n",
    "lda_model.save('C:\\\\Users\\\\schapira.d\\\\Desktop\\\\Data Science Meetup\\\\lda_5010.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_#</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.099*\"table\" + 0.024*\"door\" + 0.022*\"people\" + 0.019*\"seat\" + 0.019*\"small\" + 0.018*\"dirty\" + 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.052*\"dish\" + 0.039*\"pasta\" + 0.029*\"sauce\" + 0.026*\"italian\" + 0.016*\"restaurant\" + 0.014*\"men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.220*\"menu\" + 0.114*\"option\" + 0.102*\"item\" + 0.031*\"healthy\" + 0.030*\"choice\" + 0.028*\"veggie\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130*\"cake\" + 0.045*\"donut\" + 0.035*\"vegas\" + 0.035*\"cheesecake\" + 0.033*\"dessert\" + 0.027*\"bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.023*\"liking\" + 0.023*\"buffalo\" + 0.021*\"unusual\" + 0.016*\"ring\" + 0.013*\"miss\" + 0.012*\"holy\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.074*\"time\" + 0.045*\"year\" + 0.019*\"visit\" + 0.019*\"restaurant\" + 0.018*\"business\" + 0.018*\"day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.036*\"sweet_potato\" + 0.018*\"pudding\" + 0.018*\"period\" + 0.015*\"boss\" + 0.015*\"ice_tea\" + 0.014...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.202*\"bread\" + 0.044*\"butter\" + 0.036*\"french\" + 0.032*\"warm\" + 0.030*\"pastry\" + 0.027*\"cheese\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.148*\"friendly\" + 0.125*\"staff\" + 0.053*\"nice\" + 0.050*\"service\" + 0.043*\"great\" + 0.033*\"clean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.144*\"wife\" + 0.123*\"beer\" + 0.117*\"kid\" + 0.058*\"family\" + 0.042*\"daughter\" + 0.039*\"son\" + 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.200*\"burger\" + 0.137*\"fry\" + 0.029*\"cheese\" + 0.024*\"bun\" + 0.013*\"bacon\" + 0.013*\"onion\" + 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.069*\"flavor\" + 0.031*\"sauce\" + 0.027*\"little\" + 0.025*\"bit\" + 0.019*\"taste\" + 0.019*\"sweet\" + ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.152*\"price\" + 0.088*\"worth\" + 0.048*\"cheap\" + 0.038*\"line\" + 0.029*\"quality\" + 0.027*\"time\" + ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.151*\"small\" + 0.111*\"portion\" + 0.092*\"large\" + 0.051*\"size\" + 0.047*\"big\" + 0.044*\"huge\" + 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.058*\"order\" + 0.056*\"time\" + 0.053*\"minute\" + 0.048*\"service\" + 0.029*\"table\" + 0.026*\"server\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.022*\"heart\" + 0.014*\"winner\" + 0.014*\"pink\" + 0.013*\"cousin\" + 0.012*\"season\" + 0.011*\"life\" +...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.198*\"star\" + 0.088*\"review\" + 0.071*\"bad\" + 0.020*\"experience\" + 0.019*\"life\" + 0.019*\"thing\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.052*\"dinner\" + 0.039*\"restaurant\" + 0.028*\"meal\" + 0.022*\"table\" + 0.022*\"night\" + 0.022*\"serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.049*\"cold\" + 0.029*\"delivery\" + 0.027*\"time\" + 0.027*\"mom\" + 0.027*\"bad\" + 0.024*\"day\" + 0.019...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.118*\"sweet\" + 0.067*\"tea\" + 0.052*\"chocolate\" + 0.028*\"cookie\" + 0.025*\"cream\" + 0.021*\"fruit\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.309*\"salad\" + 0.032*\"dressing\" + 0.029*\"fresh\" + 0.027*\"wrap\" + 0.019*\"hummus\" + 0.018*\"slider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.063*\"raw\" + 0.027*\"iced_tea\" + 0.022*\"allergy\" + 0.020*\"everybody\" + 0.020*\"ceviche\" + 0.020*\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.332*\"great\" + 0.137*\"service\" + 0.049*\"excellent\" + 0.034*\"awesome\" + 0.028*\"time\" + 0.026*\"at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.086*\"soup\" + 0.058*\"noodle\" + 0.043*\"bowl\" + 0.039*\"rice\" + 0.036*\"beef\" + 0.033*\"dish\" + 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.120*\"dish\" + 0.101*\"spicy\" + 0.046*\"curry\" + 0.033*\"indian\" + 0.028*\"spice\" + 0.024*\"flavour\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.036*\"und\" + 0.017*\"ist\" + 0.017*\"das\" + 0.016*\"war\" + 0.015*\"der\" + 0.015*\"man\" + 0.015*\"nicht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.121*\"buffet\" + 0.096*\"selection\" + 0.053*\"dessert\" + 0.050*\"variety\" + 0.045*\"seafood\" + 0.021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.081*\"store\" + 0.065*\"water\" + 0.058*\"dog\" + 0.042*\"soda\" + 0.039*\"ice\" + 0.037*\"hot_dog\" + 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.056*\"coffee\" + 0.029*\"friend\" + 0.024*\"girl\" + 0.022*\"little\" + 0.022*\"guy\" + 0.021*\"drink\" + ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.120*\"breakfast\" + 0.074*\"egg\" + 0.047*\"brunch\" + 0.038*\"bacon\" + 0.030*\"pancake\" + 0.030*\"waff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.056*\"dim_sum\" + 0.035*\"brother\" + 0.023*\"cart\" + 0.022*\"foot\" + 0.017*\"closed\" + 0.015*\"spoon\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.260*\"pizza\" + 0.042*\"cheese\" + 0.032*\"topping\" + 0.031*\"slice\" + 0.029*\"crust\" + 0.024*\"pie\" +...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.173*\"meat\" + 0.068*\"pork\" + 0.041*\"bbq\" + 0.041*\"side\" + 0.038*\"rib\" + 0.035*\"sauce\" + 0.026*\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.054*\"service\" + 0.051*\"ok\" + 0.048*\"bad\" + 0.029*\"average\" + 0.026*\"bland\" + 0.026*\"okay\" + 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.070*\"fun\" + 0.043*\"group\" + 0.037*\"game\" + 0.035*\"music\" + 0.033*\"loud\" + 0.029*\"people\" + 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.156*\"amazing\" + 0.085*\"delicious\" + 0.036*\"time\" + 0.035*\"favorite\" + 0.029*\"perfect\" + 0.024*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.129*\"raman\" + 0.051*\"pork_belly\" + 0.048*\"hype\" + 0.025*\"buddy\" + 0.019*\"belly\" + 0.018*\"fianc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.159*\"sushi\" + 0.102*\"roll\" + 0.091*\"fish\" + 0.038*\"fresh\" + 0.035*\"chef\" + 0.029*\"salmon\" + 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.045*\"secret\" + 0.013*\"hookah\" + 0.013*\"bottomless\" + 0.012*\"little_pricy\" + 0.010*\"pork_bone\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.029*\"%\" + 0.026*\"free\" + 0.024*\"tip\" + 0.023*\"bill\" + 0.017*\"dollar\" + 0.015*\"card\" + 0.015*\"c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.100*\"bar\" + 0.072*\"drink\" + 0.041*\"great\" + 0.029*\"beer\" + 0.023*\"patio\" + 0.023*\"bartender\" +...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.289*\"hot\" + 0.163*\"wing\" + 0.111*\"sauce\" + 0.017*\"cold\" + 0.015*\"ranch\" + 0.014*\"pound\" + 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.196*\"steak\" + 0.071*\"shrimp\" + 0.048*\"lobster\" + 0.037*\"potato\" + 0.036*\"oyster\" + 0.030*\"crab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.211*\"lunch\" + 0.184*\"sandwich\" + 0.035*\"today\" + 0.027*\"special\" + 0.022*\"time\" + 0.019*\"day\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.322*\"location\" + 0.186*\"new\" + 0.037*\"original\" + 0.033*\"downtown\" + 0.024*\"convenient\" + 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.027*\"et\" + 0.021*\"la\" + 0.012*\"service\" + 0.012*\"mai\" + 0.010*\"pas\" + 0.010*\"e\" + 0.010*\"qui\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.050*\"room\" + 0.030*\"hotel\" + 0.026*\"parking\" + 0.020*\"strip\" + 0.017*\"view\" + 0.016*\"nice\" + 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.112*\"taco\" + 0.061*\"mexican\" + 0.044*\"burrito\" + 0.038*\"salsa\" + 0.037*\"chip\" + 0.023*\"bean\" +...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.219*\"restaurant\" + 0.042*\"authentic\" + 0.040*\"chinese\" + 0.037*\"style\" + 0.030*\"dish\" + 0.029*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.418*\"chicken\" + 0.054*\"rice\" + 0.032*\"sauce\" + 0.020*\"gyro\" + 0.017*\"meal\" + 0.015*\"piece\" + 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                    Keywords\n",
       "Topic_#                                                                                                     \n",
       "0        0.099*\"table\" + 0.024*\"door\" + 0.022*\"people\" + 0.019*\"seat\" + 0.019*\"small\" + 0.018*\"dirty\" + 0...\n",
       "1        0.052*\"dish\" + 0.039*\"pasta\" + 0.029*\"sauce\" + 0.026*\"italian\" + 0.016*\"restaurant\" + 0.014*\"men...\n",
       "2        0.220*\"menu\" + 0.114*\"option\" + 0.102*\"item\" + 0.031*\"healthy\" + 0.030*\"choice\" + 0.028*\"veggie\"...\n",
       "3        0.130*\"cake\" + 0.045*\"donut\" + 0.035*\"vegas\" + 0.035*\"cheesecake\" + 0.033*\"dessert\" + 0.027*\"bes...\n",
       "4        0.023*\"liking\" + 0.023*\"buffalo\" + 0.021*\"unusual\" + 0.016*\"ring\" + 0.013*\"miss\" + 0.012*\"holy\" ...\n",
       "5        0.074*\"time\" + 0.045*\"year\" + 0.019*\"visit\" + 0.019*\"restaurant\" + 0.018*\"business\" + 0.018*\"day...\n",
       "6        0.036*\"sweet_potato\" + 0.018*\"pudding\" + 0.018*\"period\" + 0.015*\"boss\" + 0.015*\"ice_tea\" + 0.014...\n",
       "7        0.202*\"bread\" + 0.044*\"butter\" + 0.036*\"french\" + 0.032*\"warm\" + 0.030*\"pastry\" + 0.027*\"cheese\"...\n",
       "8        0.148*\"friendly\" + 0.125*\"staff\" + 0.053*\"nice\" + 0.050*\"service\" + 0.043*\"great\" + 0.033*\"clean...\n",
       "9        0.144*\"wife\" + 0.123*\"beer\" + 0.117*\"kid\" + 0.058*\"family\" + 0.042*\"daughter\" + 0.039*\"son\" + 0....\n",
       "10       0.200*\"burger\" + 0.137*\"fry\" + 0.029*\"cheese\" + 0.024*\"bun\" + 0.013*\"bacon\" + 0.013*\"onion\" + 0....\n",
       "11       0.069*\"flavor\" + 0.031*\"sauce\" + 0.027*\"little\" + 0.025*\"bit\" + 0.019*\"taste\" + 0.019*\"sweet\" + ...\n",
       "12       0.152*\"price\" + 0.088*\"worth\" + 0.048*\"cheap\" + 0.038*\"line\" + 0.029*\"quality\" + 0.027*\"time\" + ...\n",
       "13       0.151*\"small\" + 0.111*\"portion\" + 0.092*\"large\" + 0.051*\"size\" + 0.047*\"big\" + 0.044*\"huge\" + 0....\n",
       "14       0.058*\"order\" + 0.056*\"time\" + 0.053*\"minute\" + 0.048*\"service\" + 0.029*\"table\" + 0.026*\"server\"...\n",
       "15       0.022*\"heart\" + 0.014*\"winner\" + 0.014*\"pink\" + 0.013*\"cousin\" + 0.012*\"season\" + 0.011*\"life\" +...\n",
       "16       0.198*\"star\" + 0.088*\"review\" + 0.071*\"bad\" + 0.020*\"experience\" + 0.019*\"life\" + 0.019*\"thing\" ...\n",
       "17       0.052*\"dinner\" + 0.039*\"restaurant\" + 0.028*\"meal\" + 0.022*\"table\" + 0.022*\"night\" + 0.022*\"serv...\n",
       "18       0.049*\"cold\" + 0.029*\"delivery\" + 0.027*\"time\" + 0.027*\"mom\" + 0.027*\"bad\" + 0.024*\"day\" + 0.019...\n",
       "19       0.118*\"sweet\" + 0.067*\"tea\" + 0.052*\"chocolate\" + 0.028*\"cookie\" + 0.025*\"cream\" + 0.021*\"fruit\"...\n",
       "20       0.309*\"salad\" + 0.032*\"dressing\" + 0.029*\"fresh\" + 0.027*\"wrap\" + 0.019*\"hummus\" + 0.018*\"slider...\n",
       "21       0.063*\"raw\" + 0.027*\"iced_tea\" + 0.022*\"allergy\" + 0.020*\"everybody\" + 0.020*\"ceviche\" + 0.020*\"...\n",
       "22       0.332*\"great\" + 0.137*\"service\" + 0.049*\"excellent\" + 0.034*\"awesome\" + 0.028*\"time\" + 0.026*\"at...\n",
       "23       0.086*\"soup\" + 0.058*\"noodle\" + 0.043*\"bowl\" + 0.039*\"rice\" + 0.036*\"beef\" + 0.033*\"dish\" + 0.02...\n",
       "24       0.120*\"dish\" + 0.101*\"spicy\" + 0.046*\"curry\" + 0.033*\"indian\" + 0.028*\"spice\" + 0.024*\"flavour\" ...\n",
       "25       0.036*\"und\" + 0.017*\"ist\" + 0.017*\"das\" + 0.016*\"war\" + 0.015*\"der\" + 0.015*\"man\" + 0.015*\"nicht...\n",
       "26       0.121*\"buffet\" + 0.096*\"selection\" + 0.053*\"dessert\" + 0.050*\"variety\" + 0.045*\"seafood\" + 0.021...\n",
       "27       0.081*\"store\" + 0.065*\"water\" + 0.058*\"dog\" + 0.042*\"soda\" + 0.039*\"ice\" + 0.037*\"hot_dog\" + 0.0...\n",
       "28       0.056*\"coffee\" + 0.029*\"friend\" + 0.024*\"girl\" + 0.022*\"little\" + 0.022*\"guy\" + 0.021*\"drink\" + ...\n",
       "29       0.120*\"breakfast\" + 0.074*\"egg\" + 0.047*\"brunch\" + 0.038*\"bacon\" + 0.030*\"pancake\" + 0.030*\"waff...\n",
       "30       0.056*\"dim_sum\" + 0.035*\"brother\" + 0.023*\"cart\" + 0.022*\"foot\" + 0.017*\"closed\" + 0.015*\"spoon\"...\n",
       "31       0.260*\"pizza\" + 0.042*\"cheese\" + 0.032*\"topping\" + 0.031*\"slice\" + 0.029*\"crust\" + 0.024*\"pie\" +...\n",
       "32       0.173*\"meat\" + 0.068*\"pork\" + 0.041*\"bbq\" + 0.041*\"side\" + 0.038*\"rib\" + 0.035*\"sauce\" + 0.026*\"...\n",
       "33       0.054*\"service\" + 0.051*\"ok\" + 0.048*\"bad\" + 0.029*\"average\" + 0.026*\"bland\" + 0.026*\"okay\" + 0....\n",
       "34       0.070*\"fun\" + 0.043*\"group\" + 0.037*\"game\" + 0.035*\"music\" + 0.033*\"loud\" + 0.029*\"people\" + 0.0...\n",
       "35       0.156*\"amazing\" + 0.085*\"delicious\" + 0.036*\"time\" + 0.035*\"favorite\" + 0.029*\"perfect\" + 0.024*...\n",
       "36       0.129*\"raman\" + 0.051*\"pork_belly\" + 0.048*\"hype\" + 0.025*\"buddy\" + 0.019*\"belly\" + 0.018*\"fianc...\n",
       "37       0.159*\"sushi\" + 0.102*\"roll\" + 0.091*\"fish\" + 0.038*\"fresh\" + 0.035*\"chef\" + 0.029*\"salmon\" + 0....\n",
       "38       0.045*\"secret\" + 0.013*\"hookah\" + 0.013*\"bottomless\" + 0.012*\"little_pricy\" + 0.010*\"pork_bone\" ...\n",
       "39       0.029*\"%\" + 0.026*\"free\" + 0.024*\"tip\" + 0.023*\"bill\" + 0.017*\"dollar\" + 0.015*\"card\" + 0.015*\"c...\n",
       "40       0.100*\"bar\" + 0.072*\"drink\" + 0.041*\"great\" + 0.029*\"beer\" + 0.023*\"patio\" + 0.023*\"bartender\" +...\n",
       "41       0.289*\"hot\" + 0.163*\"wing\" + 0.111*\"sauce\" + 0.017*\"cold\" + 0.015*\"ranch\" + 0.014*\"pound\" + 0.01...\n",
       "42       0.196*\"steak\" + 0.071*\"shrimp\" + 0.048*\"lobster\" + 0.037*\"potato\" + 0.036*\"oyster\" + 0.030*\"crab...\n",
       "43       0.211*\"lunch\" + 0.184*\"sandwich\" + 0.035*\"today\" + 0.027*\"special\" + 0.022*\"time\" + 0.019*\"day\" ...\n",
       "44       0.322*\"location\" + 0.186*\"new\" + 0.037*\"original\" + 0.033*\"downtown\" + 0.024*\"convenient\" + 0.02...\n",
       "45       0.027*\"et\" + 0.021*\"la\" + 0.012*\"service\" + 0.012*\"mai\" + 0.010*\"pas\" + 0.010*\"e\" + 0.010*\"qui\" ...\n",
       "46       0.050*\"room\" + 0.030*\"hotel\" + 0.026*\"parking\" + 0.020*\"strip\" + 0.017*\"view\" + 0.016*\"nice\" + 0...\n",
       "47       0.112*\"taco\" + 0.061*\"mexican\" + 0.044*\"burrito\" + 0.038*\"salsa\" + 0.037*\"chip\" + 0.023*\"bean\" +...\n",
       "48       0.219*\"restaurant\" + 0.042*\"authentic\" + 0.040*\"chinese\" + 0.037*\"style\" + 0.030*\"dish\" + 0.029*...\n",
       "49       0.418*\"chicken\" + 0.054*\"rice\" + 0.032*\"sauce\" + 0.020*\"gyro\" + 0.017*\"meal\" + 0.015*\"piece\" + 0..."
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_topics = lda_model.print_topics(num_topics=-1)\n",
    "lda_topics = pd.DataFrame(lda_topics, columns=['Topic_#','Keywords']).set_index('Topic_#')\n",
    "pd.options.display.max_colwidth=100\n",
    "lda_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TopicDetection(doc,min_topic_freq,topn):\n",
    "    \"\"\"\n",
    "    Runs LDA against a document and returns most dominant topics & top keywords\n",
    "    associated with topics. \n",
    "    \"\"\"\n",
    "    \n",
    "    doc_tokens = TextPreprocessSpaCy(doc)[0] #spaCy preprocess\n",
    "    doc_trigram = list(trigram_phraser[bigram_phraser[doc_tokens]]) # phrase model\n",
    "    doc_bow = dictionary.doc2bow(doc_trigram) #create bow representation\n",
    "    doc_lda = lda_model[doc_bow] # run LDA on doc\n",
    "    \n",
    "    #create columns for output df\n",
    "    topic_num = [x[0] for x in doc_lda]\n",
    "    topic_freq = [x[1] for x in doc_lda]\n",
    "    topic_keywords = []\n",
    "    for i in doc_lda:\n",
    "        keywords = [x[0] for x in lda_model.show_topic(i[0],topn=topn)]\n",
    "        topic_keywords.append(keywords)\n",
    "    \n",
    "    headers = ['topic_num','topic_freq','topic_keywords']\n",
    "    df = pd.DataFrame(list(zip(topic_num, topic_freq, topic_keywords)),columns=headers)\n",
    "    df = df[df.topic_freq>min_topic_freq].sort_values('topic_freq',ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 330 ms\n",
      "   topic_num  topic_freq                                 topic_keywords\n",
      "0         31    0.292946         [pizza, cheese, topping, slice, crust]\n",
      "1         17    0.145488       [dinner, restaurant, meal, table, night]\n",
      "2          0    0.117800             [table, door, people, seat, small]\n",
      "3         35    0.105540  [amazing, delicious, time, favorite, perfect]\n",
      "4         40    0.102147               [bar, drink, great, beer, patio]\n",
      "\n",
      "['OMG. This pizza is AMAZING. I honestly don\\'t think I\\'ve had better pizza anywhere else. The crust is to die for...I don\\'t know what the secret is (the dough recipe? the oven? magical pizza elves?) and I don\\'t care as long as they keep making it so I can put it my mouth...nom nom nom. They don\\'t take reservations and it can get busy on a weekend evening but you can put in your name and sit at their (very tiny) \"bar\" for a drink or head next door to Johnson Public House and they will call when your table is ready. Oh, and I have to give one suggestion to Salvatore\\'s on behalf of my husband, please put olive oil on the table so he can dip the delicious crust in it.']\n"
     ]
    }
   ],
   "source": [
    "text = [reviews[200000]]\n",
    "%time topic = TopicDetection(text,0.1,5)\n",
    "print(\"{}\\n\\n{}\".format(topic,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topic_num  topic_freq                                 topic_keywords\n",
      "0         12    0.334112           [price, worth, cheap, line, quality]\n",
      "1         37    0.319998               [sushi, roll, fish, fresh, chef]\n",
      "2         35    0.130335  [amazing, delicious, time, favorite, perfect]\n",
      "3          9    0.113333            [wife, beer, kid, family, daughter]\n",
      "\n",
      "['My son loves yoyo sushi, the rolls are amazing and fresh but prices are a bit high']\n"
     ]
    }
   ],
   "source": [
    "text = ['My son loves yoyo sushi, the rolls are amazing and fresh but prices are a bit high']\n",
    "topic = TopicDetection(text,0.1,5)\n",
    "print(\"{}\\n\\n{}\".format(topic,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topic_num  topic_freq                          topic_keywords\n",
      "0         45      0.3775             [et, la, service, mai, pas]\n",
      "1          7      0.2525   [bread, butter, french, warm, pastry]\n",
      "2         30      0.1275  [dim_sum, brother, cart, foot, closed]\n",
      "3         47      0.1275   [taco, mexican, burrito, salsa, chip]\n",
      "\n",
      "['Une baguette de pain ou simplement baguette est une varit de pain, reconnaissable  sa forme allonge']\n"
     ]
    }
   ],
   "source": [
    "text = ['Une baguette de pain ou simplement baguette est une varit de pain, reconnaissable  sa forme allonge']\n",
    "topic = TopicDetection(text,0.1,5)\n",
    "print(\"{}\\n\\n{}\".format(topic,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.4330</td>\n",
       "      <td>flavor, sauce, little, bit, taste, sweet, nice, texture, light, meat</td>\n",
       "      <td>The interior was beautiful but the food....not so much. We went during lunch and three of us had...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.7171</td>\n",
       "      <td>dish, spicy, curry, indian, spice, flavour, thai, rice, favourite, restaurant</td>\n",
       "      <td>Always generous portions of rice. Thai pineapple is hands down the best! Tuna tango is also anot...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.2048</td>\n",
       "      <td>coffee, friend, girl, little, guy, drink, nice, open, cafe, people</td>\n",
       "      <td>Yes it's only 8:45 in the morning but I am still having Rigatony's withdrawal. Can you guys ship...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>bar, drink, great, beer, patio, bartender, happy_hour, nice, night, menu</td>\n",
       "      <td>Else's the place to be. \\n\\nAnd I don't mean it in a scenester or hipster sort of way. It's just...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2076</td>\n",
       "      <td>dish, pasta, sauce, italian, restaurant, menu, cheese, mushroom, meal, dessert</td>\n",
       "      <td>I just revisited this place on Satuday.\\nI recommend that you take a tiny sample of everything i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0            11.0              0.4330   \n",
       "1            1            24.0              0.7171   \n",
       "2            2            28.0              0.2048   \n",
       "3            3            40.0              0.2114   \n",
       "4            4             1.0              0.2076   \n",
       "\n",
       "                                                                         Keywords  \\\n",
       "0            flavor, sauce, little, bit, taste, sweet, nice, texture, light, meat   \n",
       "1   dish, spicy, curry, indian, spice, flavour, thai, rice, favourite, restaurant   \n",
       "2              coffee, friend, girl, little, guy, drink, nice, open, cafe, people   \n",
       "3        bar, drink, great, beer, patio, bartender, happy_hour, nice, night, menu   \n",
       "4  dish, pasta, sauce, italian, restaurant, menu, cheese, mushroom, meal, dessert   \n",
       "\n",
       "                                                                                                  Text  \\\n",
       "0  The interior was beautiful but the food....not so much. We went during lunch and three of us had...   \n",
       "1  Always generous portions of rice. Thai pineapple is hands down the best! Tuna tango is also anot...   \n",
       "2  Yes it's only 8:45 in the morning but I am still having Rigatony's withdrawal. Can you guys ship...   \n",
       "3  Else's the place to be. \\n\\nAnd I don't mean it in a scenester or hipster sort of way. It's just...   \n",
       "4  I just revisited this place on Satuday.\\nI recommend that you take a tiny sample of everything i...   \n",
       "\n",
       "   Rating  \n",
       "0       1  \n",
       "1       5  \n",
       "2       5  \n",
       "3       5  \n",
       "4       3  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function forked from:\n",
    "#https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=reviews, ratings=ratings):\n",
    "    \"\"\"\n",
    "    Extract dominant topic from each document and append original text & rating\n",
    "    \"\"\"\n",
    "    \n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text and rating to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    ratings = pd.Series(ratings)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents, ratings], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "#Sample from original data -optional so it runs quicker-\n",
    "corpus_sample, reviews_sample, ratings_sample = zip(*random.sample(list(zip(corpus, reviews, ratings)), 20000))\n",
    "\n",
    "#Run fuction\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, \n",
    "                                                  corpus=corpus_sample, \n",
    "                                                  texts=reviews_sample,\n",
    "                                                  ratings=ratings_sample)\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text', 'Rating']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keywords_unique</th>\n",
       "      <th>Rating_len</th>\n",
       "      <th>Rating_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35.0</th>\n",
       "      <td>amazing, delicious, time, favorite, perfect, yummy, fresh, everything, wonderful, ice_cream</td>\n",
       "      <td>607</td>\n",
       "      <td>4.678748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22.0</th>\n",
       "      <td>great, service, excellent, awesome, time, atmosphere, amazing, fantastic, delicious, customer</td>\n",
       "      <td>1513</td>\n",
       "      <td>4.592201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>friendly, staff, nice, service, great, clean, attentive, delicious, fresh, atmosphere</td>\n",
       "      <td>1169</td>\n",
       "      <td>4.433704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44.0</th>\n",
       "      <td>location, new, original, downtown, convenient, drive, favorite, valley, close, fav</td>\n",
       "      <td>47</td>\n",
       "      <td>4.148936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17.0</th>\n",
       "      <td>dinner, restaurant, meal, table, night, server, service, appetizer, great, waiter</td>\n",
       "      <td>1717</td>\n",
       "      <td>4.051252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.0</th>\n",
       "      <td>sweet, tea, chocolate, cookie, cream, fruit, sugar, bomb, treat, strawberry</td>\n",
       "      <td>62</td>\n",
       "      <td>4.016129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48.0</th>\n",
       "      <td>restaurant, authentic, chinese, style, dish, pho, family, area, traditional, asian</td>\n",
       "      <td>321</td>\n",
       "      <td>4.009346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>cake, donut, vegas, cheesecake, dessert, best, unbelievable, dozen, lemon, wedding</td>\n",
       "      <td>28</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>menu, option, item, healthy, choice, veggie, vegan, vegetarian, lot, variety</td>\n",
       "      <td>189</td>\n",
       "      <td>3.989418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.0</th>\n",
       "      <td>breakfast, egg, brunch, bacon, pancake, waffle, morning, coffee, potato, sausage</td>\n",
       "      <td>640</td>\n",
       "      <td>3.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24.0</th>\n",
       "      <td>dish, spicy, curry, indian, spice, flavour, thai, rice, favourite, restaurant</td>\n",
       "      <td>249</td>\n",
       "      <td>3.947791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>bread, butter, french, warm, pastry, cheese, soft, baked, fresh, bakery</td>\n",
       "      <td>35</td>\n",
       "      <td>3.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40.0</th>\n",
       "      <td>bar, drink, great, beer, patio, bartender, happy_hour, nice, night, menu</td>\n",
       "      <td>1248</td>\n",
       "      <td>3.942308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28.0</th>\n",
       "      <td>coffee, friend, girl, little, guy, drink, nice, open, cafe, people</td>\n",
       "      <td>511</td>\n",
       "      <td>3.929550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>wife, beer, kid, family, daughter, son, child, flight, year_old, parent</td>\n",
       "      <td>54</td>\n",
       "      <td>3.925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43.0</th>\n",
       "      <td>lunch, sandwich, today, special, time, day, dinner, bagel, quick, sub</td>\n",
       "      <td>438</td>\n",
       "      <td>3.917808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37.0</th>\n",
       "      <td>sushi, roll, fish, fresh, chef, salmon, japanese, rice, sashimi, tuna</td>\n",
       "      <td>420</td>\n",
       "      <td>3.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.0</th>\n",
       "      <td>flavor, sauce, little, bit, taste, sweet, nice, texture, light, meat</td>\n",
       "      <td>669</td>\n",
       "      <td>3.863976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45.0</th>\n",
       "      <td>et, la, service, mai, pas, e, qui, c', , dan</td>\n",
       "      <td>88</td>\n",
       "      <td>3.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>dish, pasta, sauce, italian, restaurant, menu, cheese, mushroom, meal, dessert</td>\n",
       "      <td>577</td>\n",
       "      <td>3.856153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>time, year, visit, restaurant, business, day, local, thing, week, new</td>\n",
       "      <td>1028</td>\n",
       "      <td>3.852140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47.0</th>\n",
       "      <td>taco, mexican, burrito, salsa, chip, bean, cheese, margarita, nachos, tortilla</td>\n",
       "      <td>556</td>\n",
       "      <td>3.843525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>burger, fry, cheese, bun, bacon, onion, poutine, patty, onion_ring, shake</td>\n",
       "      <td>610</td>\n",
       "      <td>3.842623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31.0</th>\n",
       "      <td>pizza, cheese, topping, slice, crust, pie, sauce, fresh, time, dough</td>\n",
       "      <td>574</td>\n",
       "      <td>3.841463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25.0</th>\n",
       "      <td>und, ist, das, war, der, man, nicht, hat, mit, aber</td>\n",
       "      <td>144</td>\n",
       "      <td>3.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32.0</th>\n",
       "      <td>meat, pork, bbq, side, rib, sauce, mac_cheese, brisket, potato, cheese</td>\n",
       "      <td>238</td>\n",
       "      <td>3.802521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.0</th>\n",
       "      <td>small, portion, large, size, big, huge, price, plate, portion_size, tiny</td>\n",
       "      <td>260</td>\n",
       "      <td>3.773077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <td>salad, dressing, fresh, wrap, hummus, slider, fish_chip, green, greek, pita</td>\n",
       "      <td>170</td>\n",
       "      <td>3.752941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41.0</th>\n",
       "      <td>hot, wing, sauce, cold, ranch, pound, extra, chicken_finger, catfish, hi</td>\n",
       "      <td>61</td>\n",
       "      <td>3.721311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46.0</th>\n",
       "      <td>room, hotel, parking, strip, view, nice, beautiful, area, street, building</td>\n",
       "      <td>221</td>\n",
       "      <td>3.714932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.0</th>\n",
       "      <td>price, worth, cheap, line, quality, time, high, wait, expensive, long</td>\n",
       "      <td>491</td>\n",
       "      <td>3.700611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23.0</th>\n",
       "      <td>soup, noodle, bowl, rice, beef, dish, broth, korean, pork, shrimp</td>\n",
       "      <td>633</td>\n",
       "      <td>3.687204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26.0</th>\n",
       "      <td>buffet, selection, dessert, variety, seafood, dinner, section, station, fresh, lot</td>\n",
       "      <td>278</td>\n",
       "      <td>3.665468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49.0</th>\n",
       "      <td>chicken, rice, sauce, gyro, meal, piece, plate, beef, tender, dry</td>\n",
       "      <td>192</td>\n",
       "      <td>3.520833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42.0</th>\n",
       "      <td>steak, shrimp, lobster, potato, oyster, crab, cut, steakhouse, filet, side</td>\n",
       "      <td>93</td>\n",
       "      <td>3.440860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34.0</th>\n",
       "      <td>fun, group, game, music, loud, people, party, night, event, crowd</td>\n",
       "      <td>102</td>\n",
       "      <td>3.431373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>table, door, people, seat, small, dirty, counter, wall, booth, kitchen</td>\n",
       "      <td>368</td>\n",
       "      <td>2.975543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16.0</th>\n",
       "      <td>star, review, bad, experience, life, thing, yelp, reason, sad, people</td>\n",
       "      <td>138</td>\n",
       "      <td>2.898551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39.0</th>\n",
       "      <td>%, free, tip, bill, dollar, card, change, coupon, cash, customer</td>\n",
       "      <td>128</td>\n",
       "      <td>2.671875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.0</th>\n",
       "      <td>order, time, minute, service, table, server, customer, waitress, manager, bad</td>\n",
       "      <td>2157</td>\n",
       "      <td>2.285118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18.0</th>\n",
       "      <td>cold, delivery, time, mom, bad, day, bag, old, gross, sister</td>\n",
       "      <td>211</td>\n",
       "      <td>2.213270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33.0</th>\n",
       "      <td>service, ok, bad, average, bland, okay, disappointed, dry, decent, mediocre</td>\n",
       "      <td>709</td>\n",
       "      <td>2.211566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              Keywords_unique  \\\n",
       "Dominant_Topic                                                                                                  \n",
       "35.0              amazing, delicious, time, favorite, perfect, yummy, fresh, everything, wonderful, ice_cream   \n",
       "22.0            great, service, excellent, awesome, time, atmosphere, amazing, fantastic, delicious, customer   \n",
       "8.0                     friendly, staff, nice, service, great, clean, attentive, delicious, fresh, atmosphere   \n",
       "44.0                       location, new, original, downtown, convenient, drive, favorite, valley, close, fav   \n",
       "17.0                        dinner, restaurant, meal, table, night, server, service, appetizer, great, waiter   \n",
       "19.0                              sweet, tea, chocolate, cookie, cream, fruit, sugar, bomb, treat, strawberry   \n",
       "48.0                       restaurant, authentic, chinese, style, dish, pho, family, area, traditional, asian   \n",
       "3.0                        cake, donut, vegas, cheesecake, dessert, best, unbelievable, dozen, lemon, wedding   \n",
       "2.0                              menu, option, item, healthy, choice, veggie, vegan, vegetarian, lot, variety   \n",
       "29.0                         breakfast, egg, brunch, bacon, pancake, waffle, morning, coffee, potato, sausage   \n",
       "24.0                            dish, spicy, curry, indian, spice, flavour, thai, rice, favourite, restaurant   \n",
       "7.0                                   bread, butter, french, warm, pastry, cheese, soft, baked, fresh, bakery   \n",
       "40.0                                 bar, drink, great, beer, patio, bartender, happy_hour, nice, night, menu   \n",
       "28.0                                       coffee, friend, girl, little, guy, drink, nice, open, cafe, people   \n",
       "9.0                                   wife, beer, kid, family, daughter, son, child, flight, year_old, parent   \n",
       "43.0                                    lunch, sandwich, today, special, time, day, dinner, bagel, quick, sub   \n",
       "37.0                                    sushi, roll, fish, fresh, chef, salmon, japanese, rice, sashimi, tuna   \n",
       "11.0                                     flavor, sauce, little, bit, taste, sweet, nice, texture, light, meat   \n",
       "45.0                                                            et, la, service, mai, pas, e, qui, c', , dan   \n",
       "1.0                            dish, pasta, sauce, italian, restaurant, menu, cheese, mushroom, meal, dessert   \n",
       "5.0                                     time, year, visit, restaurant, business, day, local, thing, week, new   \n",
       "47.0                           taco, mexican, burrito, salsa, chip, bean, cheese, margarita, nachos, tortilla   \n",
       "10.0                                burger, fry, cheese, bun, bacon, onion, poutine, patty, onion_ring, shake   \n",
       "31.0                                     pizza, cheese, topping, slice, crust, pie, sauce, fresh, time, dough   \n",
       "25.0                                                      und, ist, das, war, der, man, nicht, hat, mit, aber   \n",
       "32.0                                   meat, pork, bbq, side, rib, sauce, mac_cheese, brisket, potato, cheese   \n",
       "13.0                                 small, portion, large, size, big, huge, price, plate, portion_size, tiny   \n",
       "20.0                              salad, dressing, fresh, wrap, hummus, slider, fish_chip, green, greek, pita   \n",
       "41.0                                 hot, wing, sauce, cold, ranch, pound, extra, chicken_finger, catfish, hi   \n",
       "46.0                               room, hotel, parking, strip, view, nice, beautiful, area, street, building   \n",
       "12.0                                    price, worth, cheap, line, quality, time, high, wait, expensive, long   \n",
       "23.0                                        soup, noodle, bowl, rice, beef, dish, broth, korean, pork, shrimp   \n",
       "26.0                       buffet, selection, dessert, variety, seafood, dinner, section, station, fresh, lot   \n",
       "49.0                                        chicken, rice, sauce, gyro, meal, piece, plate, beef, tender, dry   \n",
       "42.0                               steak, shrimp, lobster, potato, oyster, crab, cut, steakhouse, filet, side   \n",
       "34.0                                        fun, group, game, music, loud, people, party, night, event, crowd   \n",
       "0.0                                    table, door, people, seat, small, dirty, counter, wall, booth, kitchen   \n",
       "16.0                                    star, review, bad, experience, life, thing, yelp, reason, sad, people   \n",
       "39.0                                         %, free, tip, bill, dollar, card, change, coupon, cash, customer   \n",
       "14.0                            order, time, minute, service, table, server, customer, waitress, manager, bad   \n",
       "18.0                                             cold, delivery, time, mom, bad, day, bag, old, gross, sister   \n",
       "33.0                              service, ok, bad, average, bland, okay, disappointed, dry, decent, mediocre   \n",
       "\n",
       "                Rating_len  Rating_mean  \n",
       "Dominant_Topic                           \n",
       "35.0                   607     4.678748  \n",
       "22.0                  1513     4.592201  \n",
       "8.0                   1169     4.433704  \n",
       "44.0                    47     4.148936  \n",
       "17.0                  1717     4.051252  \n",
       "19.0                    62     4.016129  \n",
       "48.0                   321     4.009346  \n",
       "3.0                     28     4.000000  \n",
       "2.0                    189     3.989418  \n",
       "29.0                   640     3.984375  \n",
       "24.0                   249     3.947791  \n",
       "7.0                     35     3.942857  \n",
       "40.0                  1248     3.942308  \n",
       "28.0                   511     3.929550  \n",
       "9.0                     54     3.925926  \n",
       "43.0                   438     3.917808  \n",
       "37.0                   420     3.866667  \n",
       "11.0                   669     3.863976  \n",
       "45.0                    88     3.863636  \n",
       "1.0                    577     3.856153  \n",
       "5.0                   1028     3.852140  \n",
       "47.0                   556     3.843525  \n",
       "10.0                   610     3.842623  \n",
       "31.0                   574     3.841463  \n",
       "25.0                   144     3.833333  \n",
       "32.0                   238     3.802521  \n",
       "13.0                   260     3.773077  \n",
       "20.0                   170     3.752941  \n",
       "41.0                    61     3.721311  \n",
       "46.0                   221     3.714932  \n",
       "12.0                   491     3.700611  \n",
       "23.0                   633     3.687204  \n",
       "26.0                   278     3.665468  \n",
       "49.0                   192     3.520833  \n",
       "42.0                    93     3.440860  \n",
       "34.0                   102     3.431373  \n",
       "0.0                    368     2.975543  \n",
       "16.0                   138     2.898551  \n",
       "39.0                   128     2.671875  \n",
       "14.0                  2157     2.285118  \n",
       "18.0                   211     2.213270  \n",
       "33.0                   709     2.211566  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topics = pd.pivot_table(df_dominant_topic,index=['Dominant_Topic'],\n",
    "                           aggfunc={'Rating':[np.mean,len],'Keywords':np.unique})\n",
    "df_topics.columns = df_topics.columns.to_series().str.join('_')\n",
    "df = df_topics.sort_values('Rating_mean',ascending=False)\n",
    "df[df.Rating_len > 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
